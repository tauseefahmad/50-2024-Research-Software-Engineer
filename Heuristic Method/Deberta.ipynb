{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e3574b-33af-401f-8266-87892974f74e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3173c6-41e1-4fa9-90a8-a31f45c690aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Load dataset\n",
    "def load_data(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "# Text Preprocessing\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s]', '', text.lower())\n",
    "        return text\n",
    "    return \"\"\n",
    "\n",
    "# Initialize BERT model and tokenizer for embedding generation\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Generate embedding for a single sentence\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).detach()\n",
    "\n",
    "# Filter relevant papers using embeddings and cosine similarity\n",
    "def filter_relevant_papers(df, threshold=0.7):\n",
    "    target_phrases = [\"deep learning in virology \",\n",
    "    \"neural networks for epidemiology\",\n",
    "    \"deep learning in epidemiology\",\n",
    "    \"neural networks in virology\",]\n",
    "    target_embedding = sum([get_embedding(phrase) for phrase in target_phrases]) / len(target_phrases)\n",
    "\n",
    "    relevant_papers = []\n",
    "    for idx, row in df.iterrows():\n",
    "        title_embedding = get_embedding(preprocess_text(row[\"Title\"]))\n",
    "        abstract_embedding = get_embedding(preprocess_text(row[\"Abstract\"]))\n",
    "        combined_embedding = (title_embedding + abstract_embedding) / 2\n",
    "        similarity = cosine_similarity(combined_embedding, target_embedding)\n",
    "        if similarity >= threshold:\n",
    "            relevant_papers.append(row)\n",
    "    return pd.DataFrame(relevant_papers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a157f92d-d45b-462f-97be-6e1c8ccbc4c4",
   "metadata": {},
   "source": [
    "## Paper Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fba14ea-77a3-43ea-b42c-7e6e3963bf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify papers into text mining, computer vision, both, or other\n",
    "def classify_papers(df) :\n",
    "    # put title and abstract together\n",
    "    df['content']  = df['Title'] + '\\n' + df['Abstract']\n",
    "\n",
    "    # zero shot classification pipeline\n",
    "    zs_pipeline = pipeline(\"zero-shot-classification\", model=\"tasksource/deberta-small-long-nli\")\n",
    "\n",
    "    # separate independent predictions for computer vision and text mining. \n",
    "    df['cv'] = predict_labels(zs_pipeline, ['Computer Vision'], list(df['content']))\n",
    "    df['nlp'] = predict_labels(zs_pipeline, ['Text Mining'], list(df['content']))\n",
    "\n",
    "    # infer 'others' and 'both' based on computer vision and text mining labels\n",
    "    df['Category'] = 'others'\n",
    "    df.loc[df.cv > 0.5, 'Category'] = 'Computer Vision'\n",
    "    df.loc[df.nlp > 0.4, 'Category'] = 'Text Mining'\n",
    "    df.loc[(df.cv > 0.5) & (df.nlp > 0.4), 'Category'] = 'both'\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118b1298-ad1d-4728-a8a2-cdc102d9a4f8",
   "metadata": {},
   "source": [
    "## Method Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ad283d-e5a1-4c42-91c7-179d297e40e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract deep learning methods from abstracts and title\n",
    "df['Title'] = df['Title'].fillna('')\n",
    "df['Abstract'] = df['Abstract'].fillna('')\n",
    "\n",
    "# List of known deep learning methods\n",
    "deep_learning_methods = ['CNN', 'Convolutional Neural Networks', 'RNN', 'Recurrent Neural Networks', \n",
    "                         'LSTM', 'Long Short-Term Memory', 'GAN', 'Generative Adversarial Networks', \n",
    "                         'DQN', 'Deep Q-Networks', 'Reinforcement Learning'\n",
    "                         'Transformer', 'transformer|attention-based model']\n",
    "\n",
    "# Function to identify deep learning method from text (title + abstract)\n",
    "def identify_dl_method(title, abstract):\n",
    "    # Combine title and abstract for keyword extraction\n",
    "    text = title + \" \" + abstract\n",
    "    keywords = kw_model.extract_keywords(text, top_n=5)  # Extract top 5 keywords\n",
    "    keywords_list = [keyword for keyword, _ in keywords]\n",
    "    \n",
    "    # Search for deep learning methods in the keywords list\n",
    "    identified_methods = [method for method in deep_learning_methods if any(keyword.lower() in method.lower() for keyword in keywords_list)]\n",
    "    \n",
    "    return identified_methods if identified_methods else ['Unknown Method']\n",
    "    \n",
    "# Apply the method identification function\n",
    "df['Identified_DL_Method'] = df.apply(lambda row: identify_dl_method(row['Title'], row['Abstract']), axis=1)\n",
    "\n",
    "# Display the DataFrame with identified deep learning methods\n",
    "#print(df[['Title', 'Identified_DL_Method']])\n",
    "\n",
    "methods = subset_df['Identified_DL_Method'].value_counts()\n",
    "\n",
    "\n",
    "# Function to categorize based on the threshold of 0.5\n",
    "def categorize(row):\n",
    "    if row['Computer Vision'] >= 0.5 and row['Text Mining'] >= 0.5:\n",
    "        return 'both'\n",
    "    elif row['Computer Vision'] < 0.5 and row['Text Mining'] < 0.5:\n",
    "        return 'others'\n",
    "    elif row['Computer Vision'] >= 0.5 and row['Text Mining'] < 0.5:\n",
    "        return 'Computer Vision'\n",
    "    else:\n",
    "        return 'Text Mining'\n",
    "\n",
    "# Apply the function to each row and create a new column 'Category'\n",
    "df1['Category'] = df1.apply(categorize, axis=1)\n",
    "\n",
    "# Get the statistics of each category (i.e., count of occurrences)\n",
    "category_stats = df1['Category'].value_counts()\n",
    "\n",
    "# Print the stats\n",
    "print(\"Category Statistics:\")\n",
    "print(category_stats)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
