{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ea1e16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete. Results saved to filtered_and_classified_papers.csv\n",
      "Total Relevant Papers: 11221\n",
      "\n",
      "Classification Counts:\n",
      "CNN                       490\n",
      "LSTM                      150\n",
      "Transformer               114\n",
      "RNN                        68\n",
      "RNN, LSTM                  22\n",
      "CNN, LSTM                  15\n",
      "CNN, RNN                    7\n",
      "CNN, Transformer            7\n",
      "CNN, RNN, LSTM              6\n",
      "RNN, Transformer            3\n",
      "CNN, Transformer, LSTM      1\n",
      "Transformer, LSTM           1\n",
      "CNN, RNN, Transformer       1\n",
      "Name: Category, dtype: int64\n",
      "\n",
      "Method Counts:\n",
      "other              7392\n",
      "computer vision    2801\n",
      "text mining         990\n",
      "both                 38\n",
      "Name: Methods Used, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Load dataset\n",
    "def load_data(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "# Text Preprocessing\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s]', '', text.lower())\n",
    "        return text\n",
    "    return \"\"\n",
    "\n",
    "# Initialize BERT model and tokenizer for embedding generation\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Generate embedding for a single sentence\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).detach()\n",
    "\n",
    "# Filter relevant papers using embeddings and cosine similarity\n",
    "def filter_relevant_papers(df, threshold=0.7):\n",
    "    target_phrases = [\"deep learning in virology\", \"neural networks for epidemiology\"]\n",
    "    target_embedding = sum([get_embedding(phrase) for phrase in target_phrases]) / len(target_phrases)\n",
    "\n",
    "    relevant_papers = []\n",
    "    for idx, row in df.iterrows():\n",
    "        title_embedding = get_embedding(preprocess_text(row[\"Title\"]))\n",
    "        abstract_embedding = get_embedding(preprocess_text(row[\"Abstract\"]))\n",
    "        combined_embedding = (title_embedding + abstract_embedding) / 2\n",
    "        similarity = cosine_similarity(combined_embedding, target_embedding)\n",
    "        if similarity >= threshold:\n",
    "            relevant_papers.append(row)\n",
    "    return pd.DataFrame(relevant_papers)\n",
    "\n",
    "# Classify papers into text mining, computer vision, both, or other\n",
    "def classify_methodology(row):\n",
    "    text_mining_keywords = [\"nlp\", \"text mining\", \"topic modeling\", \"language model\", \"bert\"]\n",
    "    computer_vision_keywords = [\"image\", \"segmentation\", \"cnn\", \"convolutional neural network\"]\n",
    "\n",
    "    text_matches = any(word in row[\"Abstract\"].lower() or word in row[\"Title\"].lower() for word in text_mining_keywords)\n",
    "    vision_matches = any(word in row[\"Abstract\"].lower() or word in row[\"Title\"].lower() for word in computer_vision_keywords)\n",
    "\n",
    "    if text_matches and vision_matches:\n",
    "        return \"both\"\n",
    "    elif text_matches:\n",
    "        return \"text mining\"\n",
    "    elif vision_matches:\n",
    "        return \"computer vision\"\n",
    "    else:\n",
    "        return \"other\"\n",
    "\n",
    "def classify_papers(df):\n",
    "    df[\"Methods Used\"] = df.apply(classify_methodology, axis=1)\n",
    "    return df\n",
    "\n",
    "# Extract specific deep learning method names used in each paper\n",
    "def extract_methods(row):\n",
    "    methods = []\n",
    "    method_patterns = {\n",
    "        \"CNN\": r\"\\b(CNN|convolutional neural network)\\b\",\n",
    "        \"RNN\": r\"\\b(RNN|recurrent neural network)\\b\",\n",
    "        \"Transformer\": r\"\\b(transformer|attention-based model)\\b\",\n",
    "        \"LSTM\": r\"\\b(LSTM|long short-term memory)\\b\"\n",
    "    }\n",
    "    for method, pattern in method_patterns.items():\n",
    "        if re.search(pattern, row[\"Abstract\"], re.IGNORECASE) or re.search(pattern, row[\"Title\"], re.IGNORECASE):\n",
    "            methods.append(method)\n",
    "    return \", \".join(methods)\n",
    "\n",
    "def extract_methods_from_papers(df):\n",
    "    df[\"Category\"] = df.apply(extract_methods, axis=1)\n",
    "    return df\n",
    "\n",
    "# Main processing function that runs the pipeline\n",
    "def process_papers(file_path, output_path):\n",
    "    # Load data\n",
    "    df = load_data(file_path)\n",
    "    \n",
    "    # Preprocess text fields\n",
    "    df[\"Title\"] = df[\"Title\"].apply(preprocess_text)\n",
    "    df[\"Abstract\"] = df[\"Abstract\"].apply(preprocess_text)\n",
    "\n",
    "    # Filter relevant papers\n",
    "    df = filter_relevant_papers(df)\n",
    "    \n",
    "    # Classify methods\n",
    "    df = classify_papers(df)\n",
    "\n",
    "    # Extract methods used\n",
    "    df = extract_methods_from_papers(df)\n",
    "\n",
    "    # Save the result\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(\"Processing complete. Results saved to\", output_path)\n",
    "\n",
    "# Summary statistics function to provide an overview of the filtered data\n",
    "def dataset_statistics(df):\n",
    "    print(\"Total Relevant Papers:\", len(df))\n",
    "    print(\"\\nClassification Counts:\")\n",
    "    print(df[\"Category\"].value_counts())\n",
    "    print(\"\\nMethod Counts:\")\n",
    "    print(df[\"Methods Used\"].value_counts())\n",
    "\n",
    "# Run the full pipeline and print statistics\n",
    "def main():\n",
    "    # Specify input and output file paths\n",
    "    file_path = \"collection_with_abstracts.csv\"  # Replace with the path to your input dataset\n",
    "    output_path = \"filtered_and_classified_papers.csv\"\n",
    "\n",
    "    # Run the processing function\n",
    "    process_papers(file_path, output_path)\n",
    "    \n",
    "    # Load the results and display statistics\n",
    "    df_result = pd.read_csv(output_path)\n",
    "    dataset_statistics(df_result)\n",
    "\n",
    "# Execute main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa90307a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
